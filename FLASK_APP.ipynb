{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\hoops\\Desktop\\FLASK_APP\\data-science\\cannabis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>flavors</th>\n",
       "      <th>race</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>medical</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afpak</td>\n",
       "      <td>Earthy, Chemical, Pine, Spicy/Herbal</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>Relaxed, Hungry, Happy, Sleepy, Creative, Focused</td>\n",
       "      <td>Dizzy</td>\n",
       "      <td>Depression, Insomnia, Pain, Stress, Lack of Ap...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Afpak, named for its direct Afghani and Pakist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>African</td>\n",
       "      <td>Spicy/Herbal, Pungent, Earthy, Pepper</td>\n",
       "      <td>sativa</td>\n",
       "      <td>Euphoric, Happy, Creative, Energetic, Talkativ...</td>\n",
       "      <td>Dry Mouth</td>\n",
       "      <td>Depression, Pain, Stress, Lack of Appetite, Na...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>African refers to the indigenous varieties of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afwreck</td>\n",
       "      <td>Pine, Earthy, Flowery, Pungent</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>Relaxed, Happy, Creative, Uplifted, Sleepy, Eu...</td>\n",
       "      <td>Dizzy, Dry Mouth, Paranoid, Dry Eyes</td>\n",
       "      <td>Pain, Stress, Headache, Fatigue, Headaches, Mu...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Afwreck is a hybrid cross of Afghani and Train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Earthy, Woody, Pungent, Pine</td>\n",
       "      <td>sativa</td>\n",
       "      <td>Relaxed, Euphoric, Happy, Energetic, Focused, ...</td>\n",
       "      <td>Dizzy, Paranoid, Dry Eyes, Anxious</td>\n",
       "      <td>Depression, Pain, Stress, Lack of Appetite, He...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Alaska, developed by Tikun Olam, is an Israeli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alchemy</td>\n",
       "      <td>Cheese, Chemical, Sweet, Spicy/Herbal</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>Relaxed, Hungry, Uplifted, Tingly, Sleepy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Insomnia, Stress, Fatigue, Headaches, Eye Pres...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Alchemy is a 50/50 hybrid cannabis strain that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name                                flavors    race  \\\n",
       "0    Afpak   Earthy, Chemical, Pine, Spicy/Herbal  hybrid   \n",
       "1  African  Spicy/Herbal, Pungent, Earthy, Pepper  sativa   \n",
       "2  Afwreck         Pine, Earthy, Flowery, Pungent  hybrid   \n",
       "3   Alaska           Earthy, Woody, Pungent, Pine  sativa   \n",
       "4  Alchemy  Cheese, Chemical, Sweet, Spicy/Herbal  hybrid   \n",
       "\n",
       "                                            positive  \\\n",
       "0  Relaxed, Hungry, Happy, Sleepy, Creative, Focused   \n",
       "1  Euphoric, Happy, Creative, Energetic, Talkativ...   \n",
       "2  Relaxed, Happy, Creative, Uplifted, Sleepy, Eu...   \n",
       "3  Relaxed, Euphoric, Happy, Energetic, Focused, ...   \n",
       "4          Relaxed, Hungry, Uplifted, Tingly, Sleepy   \n",
       "\n",
       "                               negative  \\\n",
       "0                                 Dizzy   \n",
       "1                             Dry Mouth   \n",
       "2  Dizzy, Dry Mouth, Paranoid, Dry Eyes   \n",
       "3    Dizzy, Paranoid, Dry Eyes, Anxious   \n",
       "4                                   NaN   \n",
       "\n",
       "                                             medical  Rating  \\\n",
       "0  Depression, Insomnia, Pain, Stress, Lack of Ap...     4.2   \n",
       "1  Depression, Pain, Stress, Lack of Appetite, Na...     3.9   \n",
       "2  Pain, Stress, Headache, Fatigue, Headaches, Mu...     4.2   \n",
       "3  Depression, Pain, Stress, Lack of Appetite, He...     4.6   \n",
       "4  Insomnia, Stress, Fatigue, Headaches, Eye Pres...     4.8   \n",
       "\n",
       "                                         Description  \n",
       "0  Afpak, named for its direct Afghani and Pakist...  \n",
       "1  African refers to the indigenous varieties of ...  \n",
       "2  Afwreck is a hybrid cross of Afghani and Train...  \n",
       "3  Alaska, developed by Tikun Olam, is an Israeli...  \n",
       "4  Alchemy is a 50/50 hybrid cannabis strain that...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name            0\n",
       "flavors         0\n",
       "race            0\n",
       "positive        0\n",
       "negative       65\n",
       "medical         3\n",
       "Rating          0\n",
       "Description     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name           0\n",
       "flavors        0\n",
       "race           0\n",
       "positive       0\n",
       "negative       0\n",
       "medical        0\n",
       "Rating         0\n",
       "Description    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all text features into one string:\n",
    "\n",
    "df['combined_text'] = df.name + \" \" + df.flavors +  \" \" + df.race + \" \" + df.positive + \" \" + df.negative + \" \" + df.medical + \" \" + df.Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = nlp.Defaults.stop_words.union([\"\\xa0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "\"\"\" Update those tokens w/o stopwords\"\"\"\n",
    "for doc in tokenizer.pipe(df['combined_text'], batch_size=500):\n",
    "    \n",
    "    doc_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.text.lower() not in STOP_WORDS) & (token.is_punct == False) & (token.is_space == False):\n",
    "            doc_tokens.append(token.text.lower())\n",
    "\n",
    "    tokens.append(doc_tokens)\n",
    "\n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [afpak, earthy,, chemical,, pine,, spicy/herba...\n",
       "1    [african, spicy/herbal,, pungent,, earthy,, pe...\n",
       "2    [afwreck, pine,, earthy,, flowery,, pungent, h...\n",
       "3    [alaska, earthy,, woody,, pungent,, pine, sati...\n",
       "4    [alchemy, cheese,, chemical,, sweet,, spicy/he...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoops\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-PRON-', 'make', 'whatev'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>breed</th>\n",
       "      <th>cannabis</th>\n",
       "      <th>cross</th>\n",
       "      <th>fruity</th>\n",
       "      <th>high</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>indica</th>\n",
       "      <th>mellow</th>\n",
       "      <th>...</th>\n",
       "      <th>yield indoor</th>\n",
       "      <th>’</th>\n",
       "      <th>’s</th>\n",
       "      <th>’s aroma</th>\n",
       "      <th>’s blueberry</th>\n",
       "      <th>’s bud</th>\n",
       "      <th>’s effect</th>\n",
       "      <th>’s genetic</th>\n",
       "      <th>’s potency</th>\n",
       "      <th>’s wonder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.197079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.115572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.047025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119901</td>\n",
       "      <td>0.136058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.095836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.054611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.041529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1952 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   breed   cannabis   cross   fruity   high   hybrid  \\\n",
       "0  0.050099  0.0     0.0        0.0     0.0      0.0    0.0      0.0   \n",
       "1  0.197079  0.0     0.0        0.0     0.0      0.0    0.0      0.0   \n",
       "2  0.072476  0.0     0.0        0.0     0.0      0.0    0.0      0.0   \n",
       "3  0.115572  0.0     0.0        0.0     0.0      0.0    0.0      0.0   \n",
       "4  0.000000  0.0     0.0        0.0     0.0      0.0    0.0      0.0   \n",
       "5  0.047025  0.0     0.0        0.0     0.0      0.0    0.0      0.0   \n",
       "6  0.095836  0.0     0.0        0.0     0.0      0.0    0.0      0.0   \n",
       "7  0.000000  0.0     0.0        0.0     0.0      0.0    0.0      0.0   \n",
       "8  0.054611  0.0     0.0        0.0     0.0      0.0    0.0      0.0   \n",
       "9  0.041529  0.0     0.0        0.0     0.0      0.0    0.0      0.0   \n",
       "\n",
       "     indica   mellow    ...      yield indoor    ’        ’s  ’s aroma  \\\n",
       "0  0.000000      0.0    ...               0.0  0.0  0.000000  0.000000   \n",
       "1  0.000000      0.0    ...               0.0  0.0  0.000000  0.000000   \n",
       "2  0.000000      0.0    ...               0.0  0.0  0.000000  0.000000   \n",
       "3  0.000000      0.0    ...               0.0  0.0  0.000000  0.000000   \n",
       "4  0.000000      0.0    ...               0.0  0.0  0.000000  0.000000   \n",
       "5  0.000000      0.0    ...               0.0  0.0  0.119901  0.136058   \n",
       "6  0.142539      0.0    ...               0.0  0.0  0.000000  0.000000   \n",
       "7  0.000000      0.0    ...               0.0  0.0  0.066510  0.000000   \n",
       "8  0.000000      0.0    ...               0.0  0.0  0.000000  0.000000   \n",
       "9  0.000000      0.0    ...               0.0  0.0  0.000000  0.000000   \n",
       "\n",
       "   ’s blueberry  ’s bud  ’s effect  ’s genetic  ’s potency  ’s wonder  \n",
       "0           0.0     0.0        0.0         0.0         0.0        0.0  \n",
       "1           0.0     0.0        0.0         0.0         0.0        0.0  \n",
       "2           0.0     0.0        0.0         0.0         0.0        0.0  \n",
       "3           0.0     0.0        0.0         0.0         0.0        0.0  \n",
       "4           0.0     0.0        0.0         0.0         0.0        0.0  \n",
       "5           0.0     0.0        0.0         0.0         0.0        0.0  \n",
       "6           0.0     0.0        0.0         0.0         0.0        0.0  \n",
       "7           0.0     0.0        0.0         0.0         0.0        0.0  \n",
       "8           0.0     0.0        0.0         0.0         0.0        0.0  \n",
       "9           0.0     0.0        0.0         0.0         0.0        0.0  \n",
       "\n",
       "[10 rows x 1952 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    \n",
    "    doc = nlp(document)\n",
    "    \n",
    "    return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True)]\n",
    "# Tunning Parameters\n",
    "\n",
    "# Instantiate vectorizer object\n",
    "tfidf = TfidfVectorizer(stop_words='english', \n",
    "                        ngram_range=(1,2),\n",
    "                        max_df=.97,\n",
    "                        min_df=3,\n",
    "                        tokenizer=tokenize)\n",
    "\n",
    "# Create a vocabulary and get word counts per document\n",
    "dtm = tfidf.fit_transform(df.combined_text) # Similiar to fit_predict\n",
    "\n",
    "# Print word counts\n",
    "\n",
    "# Get feature names to use as dataframe column headers\n",
    "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
    "\n",
    "# View Feature Matrix as DataFrame\n",
    "dtm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
       "         metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Fit on DTM\n",
    "nn = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
    "nn.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = [\"Looking for something to help with depression and insomnia\"]\n",
    "user_input = tfidf.transform(test_input)\n",
    "score,strain= nn.kneighbors(user_input.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63 127 129 106 306]]\n"
     ]
    }
   ],
   "source": [
    "print(strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                                                        Django\n",
       "flavors                            Earthy, Sweet, Pungent, Flowery\n",
       "race                                                        sativa\n",
       "positive           Euphoric, Happy, Energetic, Talkative, Uplifted\n",
       "negative                                Dizzy, Dry Mouth, Dry Eyes\n",
       "medical          Depression, Insomnia, Stress, Fatigue, Muscle ...\n",
       "Rating                                                         4.6\n",
       "Description      Django means “I awake,” in Romani, and this up...\n",
       "combined_text    Django Earthy, Sweet, Pungent, Flowery sativa ...\n",
       "tokens           [django, earthy,, sweet,, pungent,, flowery, s...\n",
       "Name: 106, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "strains = [df[['name', 'medical']].loc[n] for n in strain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[              name                                            medical\n",
       " 63    Butterscotch  Depression, Insomnia, Pain, Stress, Muscle Spasms\n",
       " 127     Freezeland  Depression, Insomnia, Stress, Lack of Appetite...\n",
       " 129      Frostbite  Depression, Insomnia, Stress, Lack of Appetite...\n",
       " 106         Django  Depression, Insomnia, Stress, Fatigue, Muscle ...\n",
       " 306  Thunderstruck  Depression, Pain, Stress, Eye Pressure, Inflam...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib \n",
    "joblib.dump(nn, 'first.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf.pkl']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tfidf, \"tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hoops'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
